## Q1 - pytorch autograd

### barebones python

To train a barebones model we can use raw tensor operations or thin wrappers:

- initialize parameters using `nn.init.kaiming_normal_`; this method changes tensor in-place (like all other `nn.init` mathods); we also have manually require 
gradients on our weights using `.requires_grad_()`:

```python

    kwargs = {'dtype':dtype, 'device':device}
    
    shape = (channel_1, C, kernel_size_1, kernel_size_1)
    tensor = torch.empty(*shape, **kwargs)
    conv_w1 = nn.init.kaiming_normal_(tensor=tensor, nonlinearity='relu').requires_grad_()

```

- write a forward pass as a function using `nn.functional`: `F.linear` and `F.relu`; 
here `F.linear(x, w, b)` is just a wrapper for `x @ w.T + b`; and `F.relu` is a wrapper
for a (a bit strange) function `F.clamp(min=0)`; also we can get a convolution using `F.conv2d()`; we directly provide parameters (defined above):

```python

F.conv2d(input=x, weight=conv_w1, bias=conv_b1, padding=2)

```

- we have accuracy `46.2%`;

### module API
- instead of initializing our parameters manually we use classes from `nn.Module` like 
`nn.Conv2d`; tensors are created with `required_grad = True` by default;
- instead of creating a function for forward pass we create a class (which subclass `nn.Module`);
- we initialize layers in the constructor and create the net connectivity in `forward` method; 
- its always a bit confusing that instead of calling `forward` directly we call the class itself: `out = model(x)`;
- it turns out that `pytorch` doesn't use Kaiming initialization by default; as specified in the [documentation](https://pytorch.org/docs/stable/generated/torch.nn.Conv2d.html) it uses uniform distribution `U(-sqrt(k), sqrt(k))` for some `k`; so we need to inittialize params manually:

```python

nn.init.kaiming_normal_(self.conv1.weight)
nn.init.zeros_(self.conv1.bias)

```

### sequential API

- nothing to comment;

### resnet 

#### why preresnet?

- First of all we use here resnet v2 (He et al., 2016). They changed the residual block (see figure 1b):

> In this paper, we analyze deep residual networks by focusing on creating a “direct” path for propagating information — not only within a residual unit, but through the entire network. Our derivations reveal that if both `h(xl)` and `f(yl)` are identity mappings, the signal could be directly propagated from one unit to any other units, in both forward and backward passes. 

- They have better results for all resnets, but especially for super deep (see table 3 in the paper). On CIFAR-10 for resnet164 error decreased from 5.93% to 5.46%, but for resnet1001 it decreased from 7.61% to 4.92%. And it also turns out it's easier to train.


#### design principles

- Preresnet consists of 3 parts: head - stages (or macro-layers) - tail. 
	- Construction of the head is different from ResNet - no more aggressive downsampling, we just change number of channels. Looks like now we're interested in working with the original resolution within the first stage.   
	- We have only 3 stages instead of 4 in a regular ResNet. Each stage contains the same number of blocks. This is again different from a regular ResNet where stage 4 contains majority of blocks - probably yet again for efficiency reasons (image resolution is decreased here).
	- The other difference with macro-layers - we use much lower depth of channels (16, 32, 64 instead of 64, 128, 256 and 512). In the assignment we use even lower depth. 
	- The main difference is in block construction itself for reasones mentioned above. We use exactly the design from fig. 1b in the paper.

- There's no official implementation of preresnet in pytorch. There was implementation in [lua](https://github.com/facebookarchive/fb.resnet.torch/blob/master/models/preresnet.lua). It was [ported to pytorch](https://github.com/bearpaw/pytorch-classification/blob/master/models/cifar/preresnet.py). But main ideas of this implementation is very close to the implementation of [ResNet itself](https://github.com/pytorch/vision/blob/main/torchvision/models/resnet.py).

- Implementation proceeds as follows:
	-  First of all we need a class for a block (residual or bottleneck). In the assignment we also use a block without skip connections - just for education purposes. We use `nn.Sequential` but in pytorch implementation it's not used. The structure of the block is defined as in the paper, each block contains 2 convolution layer, some BN and ReLUs. 
	- In some blocks the first convolution layer has a `stride=2`. We know this is used for downsampling instead of max pooling. In preresnet there are only 2 such blocks - in the beginning of layers 2, 3, but not in layer 1 (we don't have downsampling over there - see above).
	- We also need a function for a macro-layer which is basically a loop over number of blocks. In the assignment we also provide the number of channels. In the actual implementation the number of channels is predefined and the same for all preresnets (the same priciple is used in a regular resnet). It turns out there's a much simpler solution - we may use `nn.AdaptiveAvgPool2d`. 


		> The layer automatically computes the kernel size so that the specified feature map size is returned. The major advantage with this layer is that whatever the input size, the output from this layer is always fixed and, hence, the neural network can accept images of any height and width. 