# Q2 - image captioning

## RNN captioning

### Image feature extraction

- In case we have no attention we just use an output of CNN as our input into RNN. We use pretrained Mobilenet with average pooling instead of classifier. 

### Word embeddings

- Our input `x` has shape `(N, T)` and contains numbers that we use to represent words in our vocabulary. `T=17` - we limit captions by 15 words and add start and end tokens, in case a caption is less than 15 words long we pad it with zeros. Again, T is the length of a caption and we supply it to RNN word by word.  
- In fact we supply not `x` itself but rather an embedding vector instead of a word. We construct an embedding matrix `W` of shape `(V, D)` where `V` - size of vocabulary, `D` - size of embedding vector (embeddings dimension). 
- How do we get an embedding for our input `x`? We use the fact that `x` contains numbers from vocabulary, so we need `(N, T)` vectors from `W`. Suppose `xij` is an element in `x`, then `W[xij]` is an embedding vector for this element. It turns out that we may use `W[x]` to get embedding of shape `(N, T, D)` for our input `x`.

### Temporal affine layer

- In RNN  we share weights between all RNN modules. So we may use the same linear layer to produce `y`. The question is - how can we deal with `h` of shape `(N, T, H)`? It turns out we can apply it to such input directly - in this case we'll get product of `Wx` for all `T` of shape `(N, T, H)`. 
- We may consider it as a multiplication of `(T, H)` and `(H, H)` matrices for each `N`. Alternatively we may consider multiplication of vector `(H,)` and this matrix for each `N, T`.

### Temporal softmax loss

- So yet again we can use pytorch built-in tools, in this case `F.cross-entropy()`. We may use it in different ways. The only thing we shoud care that vocabulary size should be preserved and be matched with `y`:

```python
F.cross_entropy(scores.permute(0, 2, 1), y, reduction='sum') / N
F.cross_entropy(scores.reshape(N*T, V), y.reshape(N*T), reduction='sum') / N
```

### `CaptioningRNN`

- Finally we have to wrap everything up into this class:
	- We use Encoder-Decoder architecture where we encode image data using CNN and then supply them into vanilla RNN. We clearly have a bottleneck here - all the information is encoded into a single vector. We use projection here.

	```python
	features = self.feature_extractor.extract_mobilenet_feature(images)
	h0 = self.feature_projection(features)
	```

	- We use embedding vectors for each word in the vocabulary. These are learnable vectors, not predefined ones (like word2vec). We use our embedding matrix here.
	- We convert hidden states of our RNN into scores over vocabulary (this is a very significant increase in dimensions). We use projection here as well.

- We have completely different forward paths for training and sampling:
	- During training we run all the modules and projections mentioned above and compute loss. 
	- During sampling we have to generate a caption for a given image. We don't have input to supply to our RNN, so we begin with `<START>` token and generate one word at a time and supply it to the next stage of our RNN: `prev_word = next_word`. We choose our next word using just maximum score (which is also a simplification): `next_word = torch.argmax(scores, dim=1)`.

### Results

So what are the results of this model? 

- It seems most of the training captions (not all of them) are close to the ground truth (GT):
	
	> Generated: `<START> a yellow train pulls up <UNK> passengers at a station <END>`. GT is exactly the same.

- Some of testing captions are somewhat reasonable:

	> Generated: `<START> a person is riding a wave in the dark <UNK> of the water <END>`. GT: `<START> a person in the ocean waves surfing alone <END>`.

## LSTM captioning (no attention)

- Basically everything is quite similar in this case. Results of the training are not quite interesting but we get the loss mentioned in the assignment.

## LSTM captioning (with attention)

### Attention computations (Scaled dot-product attention)

- We compute [classic attention](https://web.eecs.umich.edu/~justincj/slides/eecs498/498_FA2019_lecture13.pdf) in 3 steps (see p. 15): 
	1) Compute *alignment scores* `e_ti` using some function `f_att`;
	2) Compute *attention weights* `a_ti` - just normalized `e_ti` (using softmax);
	3) Compute *context vector* `c_t` using dot product of attention weight and hidden states of Encoder (in classic attention we use 2 RNNs in Encoder-Decoder model);

- To compute attention in our case we need to do the following:
	- First of all we don't use pooling on our CNN's features. But we still using a projection so we get a blob `A` of shape `(H, 4, 4)` for each example in a batch. Why do we need to project to size `H`? See below.
	- What kind of `f_att` do we use? It turns out that we use just a dot profuct of `h` - hidden states of LSTM (corresponds to Decoder in a classic seq2seq model) and this blob A. Size of `h` is of course `H` - that's the reason to project on this size.
	- In our computation we follow steps 1-3 above. There are some details regarding batch product and flattening of `A` - see detailed comments in the assignment and the code. In particular step 3 looks like (in notation from the assignment):

	```python
		# M_tilde - flattened alignment scores
		# A_tilde - flattened A blob
		attn = torch.bmm(A_tilde, M_tilde)
	```

### `lstm_step_forward`

- We just need to add our attention vector to computations. In case of LSTM we add it to activation before computing gates (with yet another projection matrix `Wattn`). This is directly specified in the assignment. 

### `lstm_forward`

- Again we just need to compute attention and use `lstm_step_forward` with attention.

### Captioning model

- In *initialization* we use no pooling (we need our blob A of shape (H, 4, 4)). For this reason we can't use linear layer anymore - we use 1x1 convolution to transform our channels.

- In *forward* our feature projector should return blob `A`, not initial state `h0`.

- Finally, in *sample* we again get `A` from projection, initialize `h0, c0` as specified in the assignment and call `step_forward` with attention.

- Do we have a better result? Slightly better for training data and complete garbage for test data. Looks like this model is not intended to get realistic captioning.

And that's it for captioning.